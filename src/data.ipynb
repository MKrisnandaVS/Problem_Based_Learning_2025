{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install pandas yfinance supabase"
      ],
      "metadata": {
        "id": "gD3p1Kj6ob32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c82fc31-7e15-439b-917e-ac7e0811cb90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.55)\n",
            "Collecting supabase\n",
            "  Downloading supabase-2.15.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.7)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.17.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.4)\n",
            "Collecting gotrue<3.0.0,>=2.11.0 (from supabase)\n",
            "  Downloading gotrue-2.12.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx<0.29,>=0.26 in /usr/local/lib/python3.11/dist-packages (from supabase) (0.28.1)\n",
            "Collecting postgrest<1.1,>0.19 (from supabase)\n",
            "  Downloading postgrest-1.0.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting realtime<2.5.0,>=2.4.0 (from supabase)\n",
            "  Downloading realtime-2.4.2-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting storage3<0.12,>=0.10 (from supabase)\n",
            "  Downloading storage3-0.11.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting supafunc<0.10,>=0.9 (from supabase)\n",
            "  Downloading supafunc-0.9.4-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.13.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.10 in /usr/local/lib/python3.11/dist-packages (from gotrue<3.0.0,>=2.11.0->supabase) (2.11.3)\n",
            "Requirement already satisfied: pyjwt<3.0.0,>=2.10.1 in /usr/local/lib/python3.11/dist-packages (from gotrue<3.0.0,>=2.11.0->supabase) (2.10.1)\n",
            "Collecting pytest-mock<4.0.0,>=3.14.0 (from gotrue<3.0.0,>=2.11.0->supabase)\n",
            "  Downloading pytest_mock-3.14.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29,>=0.26->supabase) (0.14.0)\n",
            "Collecting deprecation<3.0.0,>=2.1.0 (from postgrest<1.1,>0.19->supabase)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.11.14 in /usr/local/lib/python3.11/dist-packages (from realtime<2.5.0,>=2.4.0->supabase) (3.11.15)\n",
            "Collecting websockets<15,>=11 (from realtime<2.5.0,>=2.4.0->supabase)\n",
            "  Downloading websockets-14.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.3.0)\n",
            "Collecting strenum<0.5.0,>=0.4.15 (from supafunc<0.10,>=0.9->supabase)\n",
            "  Downloading StrEnum-0.4.15-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->realtime<2.5.0,>=2.4.0->supabase) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->realtime<2.5.0,>=2.4.0->supabase) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->realtime<2.5.0,>=2.4.0->supabase) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->realtime<2.5.0,>=2.4.0->supabase) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->realtime<2.5.0,>=2.4.0->supabase) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->realtime<2.5.0,>=2.4.0->supabase) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->realtime<2.5.0,>=2.4.0->supabase) (1.19.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from deprecation<3.0.0,>=2.1.0->postgrest<1.1,>0.19->supabase) (24.2)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]<0.29,>=0.26->gotrue<3.0.0,>=2.11.0->supabase) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10->gotrue<3.0.0,>=2.11.0->supabase) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10->gotrue<3.0.0,>=2.11.0->supabase) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10->gotrue<3.0.0,>=2.11.0->supabase) (0.4.0)\n",
            "Requirement already satisfied: pytest>=6.2.5 in /usr/local/lib/python3.11/dist-packages (from pytest-mock<4.0.0,>=3.14.0->gotrue<3.0.0,>=2.11.0->supabase) (8.3.5)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.29,>=0.26->supabase) (1.3.1)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->gotrue<3.0.0,>=2.11.0->supabase) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->gotrue<3.0.0,>=2.11.0->supabase) (4.1.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest>=6.2.5->pytest-mock<4.0.0,>=3.14.0->gotrue<3.0.0,>=2.11.0->supabase) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest>=6.2.5->pytest-mock<4.0.0,>=3.14.0->gotrue<3.0.0,>=2.11.0->supabase) (1.5.0)\n",
            "Downloading supabase-2.15.0-py3-none-any.whl (17 kB)\n",
            "Downloading gotrue-2.12.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading postgrest-1.0.1-py3-none-any.whl (22 kB)\n",
            "Downloading realtime-2.4.2-py3-none-any.whl (22 kB)\n",
            "Downloading storage3-0.11.3-py3-none-any.whl (17 kB)\n",
            "Downloading supafunc-0.9.4-py3-none-any.whl (7.8 kB)\n",
            "Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading pytest_mock-3.14.0-py3-none-any.whl (9.9 kB)\n",
            "Downloading StrEnum-0.4.15-py3-none-any.whl (8.9 kB)\n",
            "Downloading websockets-14.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (169 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m169.9/169.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: strenum, websockets, deprecation, pytest-mock, realtime, supafunc, storage3, postgrest, gotrue, supabase\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "Successfully installed deprecation-2.1.0 gotrue-2.12.0 postgrest-1.0.1 pytest-mock-3.14.0 realtime-2.4.2 storage3-0.11.3 strenum-0.4.15 supabase-2.15.0 supafunc-0.9.4 websockets-14.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Company Info"
      ],
      "metadata": {
        "id": "9oBByb4PhM5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping Data"
      ],
      "metadata": {
        "id": "UiUAZB_og_La"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import logging\n",
        "import os\n",
        "import json\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "# Configure logging for information and error tracking\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class CompanyInfoFetcher:\n",
        "    \"\"\"\n",
        "    Class for fetching company information from Yahoo Finance\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_dir: str = \"company_data\"):\n",
        "        \"\"\"\n",
        "        Initialize CompanyInfoFetcher with output directory.\n",
        "\n",
        "        Args:\n",
        "            output_dir (str): Directory to store CSV files.\n",
        "        \"\"\"\n",
        "        self.output_dir = output_dir\n",
        "\n",
        "        # Create output directory if it doesn't exist\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "            logger.info(f\"Output directory created: {output_dir}\")\n",
        "\n",
        "    def fetch_company_info(self, ticker: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Fetch company information for a given ticker.\n",
        "\n",
        "        Args:\n",
        "            ticker (str): Stock symbol (e.g., \"BBCA.JK\")\n",
        "\n",
        "        Returns:\n",
        "            Dict: Dictionary containing company information\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logger.info(f\"Fetching company info for {ticker}\")\n",
        "            stock = yf.Ticker(ticker)\n",
        "\n",
        "            # Get company information\n",
        "            info = stock.info\n",
        "\n",
        "            # Extract ONLY the requested information\n",
        "            company_data = {\n",
        "                'ticker': ticker,\n",
        "                'address1': info.get('address1', None),\n",
        "                'sector': info.get('sector', None),\n",
        "                'website': info.get('website', None),\n",
        "                'phone': info.get('phone', None),\n",
        "                'longname': info.get('longName', None),\n",
        "                'longbusinesssummary': info.get('longBusinessSummary', None)\n",
        "            }\n",
        "\n",
        "            logger.info(f\"Successfully fetched company info for {ticker}\")\n",
        "            return company_data\n",
        "\n",
        "        except Exception as ex:\n",
        "            logger.exception(f\"Failed to fetch company info for {ticker}: {ex}\")\n",
        "            return {\n",
        "                'ticker': ticker,\n",
        "                'error': str(ex)\n",
        "            }\n",
        "\n",
        "    def save_data_to_csv(self, data: Dict, ticker: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Save company data to CSV file.\n",
        "\n",
        "        Args:\n",
        "            data (Dict): Company information dictionary\n",
        "            ticker (str): Stock symbol\n",
        "\n",
        "        Returns:\n",
        "            Optional[str]: Path to CSV file if successful, None if failed\n",
        "        \"\"\"\n",
        "        if not data or (len(data) <= 2 and 'error' in data):\n",
        "            logger.warning(f\"No data to save for {ticker}\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # Clean ticker for filename\n",
        "            ticker_clean = ticker.replace('.', '_')\n",
        "            filename = f\"{ticker_clean}_company_info.csv\"\n",
        "            filepath = os.path.join(self.output_dir, filename)\n",
        "\n",
        "            # Convert to DataFrame and save\n",
        "            df = pd.DataFrame([data])\n",
        "            df.to_csv(filepath, index=False)\n",
        "\n",
        "            # Also save as JSON for easier debugging\n",
        "            json_path = os.path.join(self.output_dir, f\"{ticker_clean}_company_info.json\")\n",
        "            with open(json_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "            logger.info(f\"Data successfully saved: {filepath}\")\n",
        "            return filepath\n",
        "\n",
        "        except Exception as ex:\n",
        "            logger.exception(f\"Failed to save data to file for {ticker}: {ex}\")\n",
        "            return None\n",
        "\n",
        "    def fetch_and_save_for_ticker(self, ticker: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Fetch and save company info for one ticker.\n",
        "\n",
        "        Args:\n",
        "            ticker (str): Stock symbol\n",
        "\n",
        "        Returns:\n",
        "            Dict: Dictionary with results\n",
        "        \"\"\"\n",
        "        data = self.fetch_company_info(ticker)\n",
        "        file_path = self.save_data_to_csv(data, ticker)\n",
        "\n",
        "        return {\n",
        "            'ticker': ticker,\n",
        "            'file_path': file_path,\n",
        "            'success': file_path is not None\n",
        "        }\n",
        "\n",
        "    def fetch_multi_tickers(self, tickers: List[str], use_threads: bool = True) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Fetch company information for multiple tickers.\n",
        "\n",
        "        Args:\n",
        "            tickers (List[str]): List of stock symbols\n",
        "            use_threads (bool): Use multithreading for data fetching\n",
        "\n",
        "        Returns:\n",
        "            List[Dict]: List of results for each ticker\n",
        "        \"\"\"\n",
        "        logger.info(f\"Starting data fetching for {len(tickers)} tickers\")\n",
        "        results = []\n",
        "\n",
        "        if use_threads:\n",
        "            # Use ThreadPoolExecutor for parallel execution\n",
        "            with ThreadPoolExecutor(max_workers=min(10, len(tickers))) as executor:\n",
        "                futures = {\n",
        "                    executor.submit(self.fetch_and_save_for_ticker, ticker): ticker\n",
        "                    for ticker in tickers\n",
        "                }\n",
        "\n",
        "                for future in futures:\n",
        "                    ticker = futures[future]\n",
        "                    try:\n",
        "                        results.append(future.result())\n",
        "                    except Exception as ex:\n",
        "                        logger.exception(f\"Error in thread for ticker {ticker}: {ex}\")\n",
        "                        results.append({\n",
        "                            'ticker': ticker,\n",
        "                            'file_path': None,\n",
        "                            'success': False,\n",
        "                            'error': str(ex)\n",
        "                        })\n",
        "        else:\n",
        "            # Sequential approach\n",
        "            for ticker in tickers:\n",
        "                results.append(self.fetch_and_save_for_ticker(ticker))\n",
        "\n",
        "        logger.info(f\"Data fetching completed\")\n",
        "        return results\n",
        "\n",
        "    def generate_summary(self, results: List[Dict]) -> None:\n",
        "        \"\"\"\n",
        "        Print a summary of data fetching results.\n",
        "\n",
        "        Args:\n",
        "            results (List[Dict]): Results from fetch_multi_tickers\n",
        "        \"\"\"\n",
        "        success_count = sum(1 for r in results if r.get('success', False))\n",
        "        failed_count = len(results) - success_count\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"COMPANY INFORMATION FETCHING SUMMARY\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        for result in results:\n",
        "            status = \"SUCCESS\" if result.get('success', False) else \"FAILED\"\n",
        "            print(f\"{result['ticker']}: {status} {result.get('file_path', '')}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"Total: {len(results)} tickers\")\n",
        "        print(f\"Success: {success_count}, Failed: {failed_count}\")\n",
        "        print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "def main():\n",
        "    # List of stocks to fetch data for\n",
        "    tickers = [\"PGEO.JK\", \"ANTM.JK\", \"PTBA.JK\", \"FCX\", \"TINS.JK\",\n",
        "               \"BBRI.JK\", \"BMRI.JK\", \"BBNI.JK\", \"INCO.JK\", \"TLKM.JK\"]\n",
        "\n",
        "    # Directory to save results\n",
        "    output_dir = \"company_data\"\n",
        "\n",
        "    # Initialize fetcher\n",
        "    fetcher = CompanyInfoFetcher(output_dir=output_dir)\n",
        "\n",
        "    # Fetch and save data\n",
        "    results = fetcher.fetch_multi_tickers(\n",
        "        tickers=tickers,\n",
        "        use_threads=True\n",
        "    )\n",
        "\n",
        "    # Display summary\n",
        "    fetcher.generate_summary(results)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb2dvSJVhRQU",
        "outputId": "bb689922-2517-4316-b864-3f62729b654d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "COMPANY INFORMATION FETCHING SUMMARY\n",
            "================================================================================\n",
            "PGEO.JK: SUCCESS company_data/PGEO_JK_company_info.csv\n",
            "ANTM.JK: SUCCESS company_data/ANTM_JK_company_info.csv\n",
            "PTBA.JK: SUCCESS company_data/PTBA_JK_company_info.csv\n",
            "FCX: SUCCESS company_data/FCX_company_info.csv\n",
            "TINS.JK: SUCCESS company_data/TINS_JK_company_info.csv\n",
            "BBRI.JK: SUCCESS company_data/BBRI_JK_company_info.csv\n",
            "BMRI.JK: SUCCESS company_data/BMRI_JK_company_info.csv\n",
            "BBNI.JK: SUCCESS company_data/BBNI_JK_company_info.csv\n",
            "INCO.JK: SUCCESS company_data/INCO_JK_company_info.csv\n",
            "TLKM.JK: SUCCESS company_data/TLKM_JK_company_info.csv\n",
            "\n",
            "================================================================================\n",
            "Total: 10 tickers\n",
            "Success: 10, Failed: 0\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-processing Data"
      ],
      "metadata": {
        "id": "QK2aFGPTkHis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Directory where CSV files are stored\n",
        "data_dir = \"company_data/\"\n",
        "\n",
        "# Find all CSV files in directory\n",
        "csv_files = glob.glob(os.path.join(data_dir, \"*_company_info.csv\"))\n",
        "\n",
        "# Loop through all CSV files\n",
        "for file_path in csv_files:\n",
        "    try:\n",
        "        # Read CSV file\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Extract ticker from filename\n",
        "        file_name = os.path.basename(file_path)\n",
        "        ticker_part = file_name.split('_company_info.csv')[0]\n",
        "\n",
        "        # Reconstruct ticker if needed\n",
        "        if 'JK' in ticker_part:\n",
        "            ticker = ticker_part.replace('_', '.')\n",
        "        else:\n",
        "            ticker = ticker_part\n",
        "\n",
        "        # Ensure ticker is in the data\n",
        "        if 'ticker' not in df.columns or df['ticker'].iloc[0] != ticker:\n",
        "            df['ticker'] = ticker\n",
        "\n",
        "        # Clean up text fields - remove newlines and excessive spaces\n",
        "        for col in ['longbusinesssummary', 'address1', 'longname']:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].astype(str).str.replace('\\n', ' ').str.replace('\\r', ' ')\n",
        "                df[col] = df[col].str.replace('nan', '').str.strip()\n",
        "                df[col] = df[col].str.replace('\\s+', ' ', regex=True)\n",
        "\n",
        "        # Handle phone number formatting if needed\n",
        "        if 'phone' in df.columns:\n",
        "            df['phone'] = df['phone'].astype(str).str.replace('nan', '')\n",
        "\n",
        "        # Handle website URL\n",
        "        if 'website' in df.columns:\n",
        "            df['website'] = df['website'].astype(str).str.replace('nan', '')\n",
        "\n",
        "        # Handle sector\n",
        "        if 'sector' in df.columns:\n",
        "            df['sector'] = df['sector'].astype(str).str.replace('nan', '')\n",
        "\n",
        "        # Ensure all expected columns exist\n",
        "        required_columns = ['ticker', 'address1', 'sector', 'website', 'phone', 'longname', 'longbusinesssummary']\n",
        "        for col in required_columns:\n",
        "            if col not in df.columns:\n",
        "                df[col] = ''\n",
        "\n",
        "        # Keep only the required columns\n",
        "        df = df[required_columns]\n",
        "\n",
        "        # Save processed CSV\n",
        "        df.to_csv(file_path, index=False)\n",
        "        print(f\"Preprocessing successful: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {str(e)}\")\n",
        "\n",
        "print(\"\\nPreprocessing completed for all files!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByihBsxXhXOn",
        "outputId": "09114716-4ab7-4782-b2f5-043ee87976dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing successful: company_data/BMRI_JK_company_info.csv\n",
            "Preprocessing successful: company_data/PGEO_JK_company_info.csv\n",
            "Preprocessing successful: company_data/BBNI_JK_company_info.csv\n",
            "Preprocessing successful: company_data/PTBA_JK_company_info.csv\n",
            "Preprocessing successful: company_data/BBRI_JK_company_info.csv\n",
            "Preprocessing successful: company_data/INCO_JK_company_info.csv\n",
            "Preprocessing successful: company_data/ANTM_JK_company_info.csv\n",
            "Preprocessing successful: company_data/TINS_JK_company_info.csv\n",
            "Preprocessing successful: company_data/FCX_company_info.csv\n",
            "Preprocessing successful: company_data/TLKM_JK_company_info.csv\n",
            "\n",
            "Preprocessing completed for all files!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Push to Supabase"
      ],
      "metadata": {
        "id": "gsAvQCWykS03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import json\n",
        "from supabase import create_client, Client\n",
        "import time\n",
        "from google.colab import userdata\n",
        "\n",
        "# Supabase credentials\n",
        "supabase_url = userdata.get('SUPABASE_URL')\n",
        "supabase_key = userdata.get('SUPABASE_KEY')\n",
        "\n",
        "# Initialize Supabase client\n",
        "supabase: Client = create_client(supabase_url, supabase_key)\n",
        "\n",
        "print(\"Supabase client initialized successfully!\")\n",
        "\n",
        "def clean_record(record):\n",
        "    \"\"\"Clean a record to ensure it can be properly handled.\"\"\"\n",
        "    # Define the required fields\n",
        "    required_fields = ['ticker', 'address1', 'sector', 'website', 'phone', 'longname', 'longbusinesssummary']\n",
        "\n",
        "    clean_data = {}\n",
        "\n",
        "    # Only keep the required fields\n",
        "    for key in required_fields:\n",
        "        value = record.get(key)\n",
        "\n",
        "        # Handle NaN, None, and other problematic values\n",
        "        if value is None or (isinstance(value, str) and value.lower() == 'nan'):\n",
        "            clean_data[key] = None\n",
        "        elif isinstance(value, float) and (value != value or value == float('inf') or value == float('-inf')):\n",
        "            # Check for NaN or infinity\n",
        "            clean_data[key] = None\n",
        "        else:\n",
        "            clean_data[key] = value\n",
        "\n",
        "    return clean_data\n",
        "\n",
        "def upload_to_supabase_from_json(json_file, table_name='company_info'):\n",
        "    \"\"\"Upload company data from JSON file to Supabase.\"\"\"\n",
        "    try:\n",
        "        # Read JSON file\n",
        "        with open(json_file, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Clean the record and keep only required fields\n",
        "        record = clean_record(data)\n",
        "        ticker = record.get('ticker')\n",
        "\n",
        "        if not ticker:\n",
        "            print(f\"No ticker found in {json_file}, skipping\")\n",
        "            return False\n",
        "\n",
        "        # Check if ticker already exists\n",
        "        response = supabase.table(table_name).select('*').eq('ticker', ticker).execute()\n",
        "\n",
        "        if hasattr(response, 'data') and response.data and len(response.data) > 0:\n",
        "            # Update existing record\n",
        "            print(f\"Updating record for {ticker}...\")\n",
        "            update_response = supabase.table(table_name).update(record).eq('ticker', ticker).execute()\n",
        "            if hasattr(update_response, 'error') and update_response.error:\n",
        "                print(f\"Error updating record for {ticker}: {update_response.error}\")\n",
        "                return False\n",
        "            else:\n",
        "                print(f\"âœ… Updated record for {ticker}\")\n",
        "                return True\n",
        "        else:\n",
        "            # Insert new record\n",
        "            print(f\"Inserting new record for {ticker}...\")\n",
        "            insert_response = supabase.table(table_name).insert(record).execute()\n",
        "            if hasattr(insert_response, 'error') and insert_response.error:\n",
        "                print(f\"Error inserting record for {ticker}: {insert_response.error}\")\n",
        "                return False\n",
        "            else:\n",
        "                print(f\"âœ… Inserted new record for {ticker}\")\n",
        "                return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error processing {json_file}: {e}\")\n",
        "        return False\n",
        "\n",
        "def upload_all_company_data_json(base_dir='company_data'):\n",
        "    \"\"\"Find and upload all company info JSON files.\"\"\"\n",
        "    total_records = 0\n",
        "    json_files = glob.glob(f\"{base_dir}/*_company_info.json\")\n",
        "\n",
        "    for json_file in json_files:\n",
        "        filename = os.path.basename(json_file)\n",
        "        print(f\"\\nðŸ”„ Processing {filename}\")\n",
        "\n",
        "        if upload_to_supabase_from_json(json_file):\n",
        "            total_records += 1\n",
        "\n",
        "        # Small delay to prevent rate limiting\n",
        "        time.sleep(0.2)\n",
        "\n",
        "    print(f\"\\nâœ… Total records uploaded to Supabase: {total_records}\")\n",
        "    return total_records\n",
        "\n",
        "# Run the upload function using JSON files\n",
        "result = upload_all_company_data_json()\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIS7i9Rchch0",
        "outputId": "636a448a-f44c-44a9-db5a-fbf1fffcdf9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Supabase client initialized successfully!\n",
            "\n",
            "ðŸ”„ Processing TLKM_JK_company_info.json\n",
            "Updating record for TLKM.JK...\n",
            "âœ… Updated record for TLKM.JK\n",
            "\n",
            "ðŸ”„ Processing PGEO_JK_company_info.json\n",
            "Updating record for PGEO.JK...\n",
            "âœ… Updated record for PGEO.JK\n",
            "\n",
            "ðŸ”„ Processing BMRI_JK_company_info.json\n",
            "Updating record for BMRI.JK...\n",
            "âœ… Updated record for BMRI.JK\n",
            "\n",
            "ðŸ”„ Processing INCO_JK_company_info.json\n",
            "Updating record for INCO.JK...\n",
            "âœ… Updated record for INCO.JK\n",
            "\n",
            "ðŸ”„ Processing PTBA_JK_company_info.json\n",
            "Updating record for PTBA.JK...\n",
            "âœ… Updated record for PTBA.JK\n",
            "\n",
            "ðŸ”„ Processing TINS_JK_company_info.json\n",
            "Updating record for TINS.JK...\n",
            "âœ… Updated record for TINS.JK\n",
            "\n",
            "ðŸ”„ Processing BBNI_JK_company_info.json\n",
            "Updating record for BBNI.JK...\n",
            "âœ… Updated record for BBNI.JK\n",
            "\n",
            "ðŸ”„ Processing BBRI_JK_company_info.json\n",
            "Updating record for BBRI.JK...\n",
            "âœ… Updated record for BBRI.JK\n",
            "\n",
            "ðŸ”„ Processing ANTM_JK_company_info.json\n",
            "Updating record for ANTM.JK...\n",
            "âœ… Updated record for ANTM.JK\n",
            "\n",
            "ðŸ”„ Processing FCX_company_info.json\n",
            "Updating record for FCX...\n",
            "âœ… Updated record for FCX\n",
            "\n",
            "âœ… Total records uploaded to Supabase: 10\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Price History"
      ],
      "metadata": {
        "id": "R2KvDss9hJR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping Data"
      ],
      "metadata": {
        "id": "50c2BXHAn1bD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import logging\n",
        "import os\n",
        "from math import ceil\n",
        "from datetime import datetime\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "# Konfigurasi logging untuk pencatatan informasi dan error\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class StockDataFetcher:\n",
        "    \"\"\"\n",
        "    Kelas untuk mengambil data historis saham dari Yahoo Finance dengan dukungan\n",
        "    untuk multi-timeframe dan multiple tickers.\n",
        "    \"\"\"\n",
        "    # Dictionary untuk memetakan interval ke perkiraan jumlah bars per hari\n",
        "    BARS_PER_DAY = {\n",
        "        '1m': 390,  # 6.5 jam perdagangan Ã— 60 menit\n",
        "        '2m': 195,  # 6.5 jam perdagangan Ã— 30 menit\n",
        "        '5m': 78,   # 6.5 jam perdagangan Ã— 12 menit\n",
        "        '15m': 26,  # 6.5 jam perdagangan Ã· 15 menit\n",
        "        '30m': 13,  # 6.5 jam perdagangan Ã· 30 menit\n",
        "        '60m': 7,   # 6.5 jam perdagangan Ã· 60 menit\n",
        "        '1h': 7,    # 6.5 jam perdagangan Ã· 1 jam\n",
        "        '1d': 1     # 1 bar per hari\n",
        "    }\n",
        "\n",
        "    # Faktor koreksi berdasarkan data aktual vs teoretis\n",
        "    CORRECTION_FACTORS = {\n",
        "        '15m': 0.78,  # 33/42\n",
        "        '30m': 0.88,  # 37/42\n",
        "        '1h': 0.95,   # 40/42\n",
        "        '1d': 1.0\n",
        "    }\n",
        "\n",
        "    # Estimasi jumlah maksimum baris per interval (berdasarkan data aktual)\n",
        "    MAX_ROWS_ESTIMATION = {\n",
        "        '15m': 856,  # Dari data pengguna\n",
        "        '30m': 482,  # Dari data pengguna\n",
        "        '1h': 283,   # Dari data pengguna\n",
        "        '1d': 1000   # Tidak terbatas oleh 60 hari\n",
        "    }\n",
        "\n",
        "    # Interval intraday yang dibatasi oleh 60 hari\n",
        "    INTRADAY_INTERVALS = ['1m', '2m', '5m', '15m', '30m', '60m', '1h']\n",
        "\n",
        "    # Perkiraan jumlah hari trading dalam 60 hari kalender (disesuaikan dari data aktual)\n",
        "    TRADING_DAYS_IN_60_CALENDAR_DAYS = 35  # Rata-rata dari 33, 37, dan 40\n",
        "\n",
        "    def __init__(self, output_dir: str = \"stock_data\"):\n",
        "        \"\"\"\n",
        "        Inisialisasi StockDataFetcher dengan direktori output.\n",
        "\n",
        "        Args:\n",
        "            output_dir (str): Direktori untuk menyimpan file CSV.\n",
        "        \"\"\"\n",
        "        self.output_dir = output_dir\n",
        "\n",
        "        # Buat direktori output jika belum ada\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "            logger.info(f\"Direktori output dibuat: {output_dir}\")\n",
        "\n",
        "    def get_max_rows_for_interval(self, interval: str) -> int:\n",
        "        \"\"\"\n",
        "        Mengembalikan perkiraan jumlah maksimum baris yang dapat diambil untuk interval\n",
        "        berdasarkan data aktual.\n",
        "\n",
        "        Args:\n",
        "            interval (str): Interval data (contoh: '15m')\n",
        "\n",
        "        Returns:\n",
        "            int: Jumlah maksimum baris data yang dapat diambil\n",
        "        \"\"\"\n",
        "        if interval in self.MAX_ROWS_ESTIMATION:\n",
        "            # Gunakan nilai dari data aktual jika tersedia\n",
        "            return self.MAX_ROWS_ESTIMATION[interval]\n",
        "        elif interval in self.INTRADAY_INTERVALS:\n",
        "            # Untuk interval intraday lainnya, hitung dengan faktor koreksi\n",
        "            avg_bars_per_day = self.BARS_PER_DAY.get(interval, 7)\n",
        "            correction = self.CORRECTION_FACTORS.get(interval, 0.85)  # Default 0.85 jika tidak ada faktor khusus\n",
        "\n",
        "            # Hitung jumlah maksimum baris dengan koreksi\n",
        "            return int(avg_bars_per_day * self.TRADING_DAYS_IN_60_CALENDAR_DAYS * correction)\n",
        "        else:\n",
        "            # Untuk data non-intraday, tidak ada batasan khusus\n",
        "            return float('inf')  # Tidak terbatas\n",
        "\n",
        "    def calculate_period(self, row_limit: int, interval: str) -> Tuple[str, int]:\n",
        "        \"\"\"\n",
        "        Menghitung periode pengambilan data berdasarkan row_limit dan interval.\n",
        "        Untuk data intraday, yfinance hanya mendukung maksimal 60 hari.\n",
        "\n",
        "        Args:\n",
        "            row_limit (int): Jumlah baris data yang diinginkan.\n",
        "            interval (str): Interval data (contoh: '15m').\n",
        "\n",
        "        Returns:\n",
        "            Tuple[str, int]:\n",
        "            - Nilai period yang valid untuk yfinance\n",
        "            - Jumlah baris yang sebenarnya akan diminta (disesuaikan jika melebihi batas)\n",
        "        \"\"\"\n",
        "        # Untuk interval intraday, periksa batas maksimum\n",
        "        if interval in self.INTRADAY_INTERVALS:\n",
        "            max_rows = self.get_max_rows_for_interval(interval)\n",
        "\n",
        "            # Jika row_limit melebihi maksimum yang mungkin dalam 60 hari\n",
        "            if row_limit > max_rows:\n",
        "                adjusted_row_limit = max_rows\n",
        "                logger.info(f\"Menyesuaikan row_limit untuk {interval} dari {row_limit} ke {adjusted_row_limit} (batas data aktual)\")\n",
        "            else:\n",
        "                adjusted_row_limit = row_limit\n",
        "\n",
        "            # Hitung jumlah hari yang diperlukan (tidak lebih dari 60)\n",
        "            # Gunakan perkiraan hari trading berdasarkan data aktual\n",
        "            if interval == '15m':\n",
        "                estimated_days = min(60, ceil(adjusted_row_limit / self.BARS_PER_DAY[interval] * (1 / self.CORRECTION_FACTORS[interval])))\n",
        "            elif interval == '30m':\n",
        "                estimated_days = min(60, ceil(adjusted_row_limit / self.BARS_PER_DAY[interval] * (1 / self.CORRECTION_FACTORS[interval])))\n",
        "            elif interval == '1h':\n",
        "                estimated_days = min(60, ceil(adjusted_row_limit / self.BARS_PER_DAY[interval] * (1 / self.CORRECTION_FACTORS[interval])))\n",
        "            else:\n",
        "                avg_bars_per_day = self.BARS_PER_DAY.get(interval, 7)\n",
        "                correction = self.CORRECTION_FACTORS.get(interval, 0.85)\n",
        "                estimated_days = min(60, ceil(adjusted_row_limit / avg_bars_per_day * (1 / correction)))\n",
        "\n",
        "            return f\"{estimated_days}d\", adjusted_row_limit\n",
        "        else:\n",
        "            # Untuk interval daily atau lebih besar\n",
        "            days_needed = ceil(row_limit / self.BARS_PER_DAY.get(interval, 1))\n",
        "\n",
        "            # Konversi ke periode yang sesuai\n",
        "            if days_needed <= 60:\n",
        "                return f\"{days_needed}d\", row_limit\n",
        "            elif days_needed <= 730:  # ~2 tahun\n",
        "                return f\"{ceil(days_needed/30)}mo\", row_limit\n",
        "            else:\n",
        "                # Untuk data sangat panjang\n",
        "                return \"max\", row_limit\n",
        "\n",
        "    def fetch_stock_data(self, ticker: str, interval: str, row_limit: int) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Mengambil data historis saham untuk ticker yang diberikan.\n",
        "\n",
        "        Args:\n",
        "            ticker (str): Simbol saham (misalnya, \"BBCA.JK\")\n",
        "            interval (str): Interval data (contoh: '15m')\n",
        "            row_limit (int): Maksimal jumlah baris data yang diinginkan.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: DataFrame yang berisi data saham yang diambil.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            stock = yf.Ticker(ticker)\n",
        "\n",
        "            # Dapatkan periode yang valid dan adjusted row limit\n",
        "            period, adjusted_row_limit = self.calculate_period(row_limit, interval)\n",
        "            logger.info(f\"Mengambil data {ticker} dengan period='{period}' dan interval='{interval}'\")\n",
        "\n",
        "            data = stock.history(period=period, interval=interval)\n",
        "\n",
        "            if data.empty:\n",
        "                logger.error(f\"Data retrieval unsuccessful for {ticker} with interval {interval}: Dataset kosong.\")\n",
        "                return pd.DataFrame()\n",
        "\n",
        "            # Log informasi tentang jumlah baris yang berhasil diambil\n",
        "            logger.info(f\"Berhasil mengambil {len(data)} baris data untuk {ticker} ({interval})\")\n",
        "\n",
        "            # Jika data yang diambil lebih banyak dari adjusted_row_limit, ambil baris terakhir sesuai limit\n",
        "            if len(data) > adjusted_row_limit:\n",
        "                data = data.tail(adjusted_row_limit)\n",
        "\n",
        "            # Tambahkan kolom Ticker untuk identifikasi\n",
        "            data['Ticker'] = ticker\n",
        "\n",
        "            # Reset index agar tanggal menjadi kolom reguler\n",
        "            data = data.reset_index()\n",
        "\n",
        "            return data\n",
        "\n",
        "        except Exception as ex:\n",
        "            logger.exception(f\"Gagal mengambil data saham {ticker} ({interval}): {ex}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def save_data_to_csv(self, df: pd.DataFrame, ticker: str, interval: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Menyimpan data ke file CSV.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): DataFrame yang akan disimpan\n",
        "            ticker (str): Simbol saham\n",
        "            interval (str): Interval data\n",
        "\n",
        "        Returns:\n",
        "            Optional[str]: Path ke file CSV jika berhasil, None jika gagal\n",
        "        \"\"\"\n",
        "        if df.empty:\n",
        "            logger.warning(f\"Tidak ada data untuk disimpan: {ticker} ({interval})\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # Format nama file: TICKER_INTERVAL.csv (contoh: BBCA_JK_1d.csv)\n",
        "            ticker_clean = ticker.replace('.', '_')  # Ganti titik dengan underscore untuk nama file\n",
        "            filename = f\"{ticker_clean}_{interval}.csv\"\n",
        "            filepath = os.path.join(self.output_dir, filename)\n",
        "\n",
        "            df.to_csv(filepath, index=False)\n",
        "            logger.info(f\"Data berhasil disimpan: {filepath} ({len(df)} baris)\")\n",
        "            return filepath\n",
        "        except Exception as ex:\n",
        "            logger.exception(f\"Gagal menyimpan data ke file untuk {ticker} ({interval}): {ex}\")\n",
        "            return None\n",
        "\n",
        "    def fetch_and_save_for_ticker(self, ticker: str, intervals: List[str], row_limit: int) -> Dict[str, Optional[str]]:\n",
        "        \"\"\"\n",
        "        Mengambil dan menyimpan data untuk satu ticker dengan beberapa interval.\n",
        "\n",
        "        Args:\n",
        "            ticker (str): Simbol saham\n",
        "            intervals (List[str]): List interval data yang akan diambil\n",
        "            row_limit (int): Maksimal jumlah baris data\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, Optional[str]]: Dictionary dengan interval sebagai key dan path file sebagai value\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "\n",
        "        for interval in intervals:\n",
        "            df = self.fetch_stock_data(ticker, interval, row_limit)\n",
        "            file_path = self.save_data_to_csv(df, ticker, interval)\n",
        "            results[interval] = file_path\n",
        "\n",
        "        return results\n",
        "\n",
        "    def fetch_multi_ticker_multi_timeframe(self,\n",
        "                                          tickers: List[str],\n",
        "                                          intervals: List[str],\n",
        "                                          row_limit: int,\n",
        "                                          use_threads: bool = True) -> Dict[str, Dict[str, Optional[str]]]:\n",
        "        \"\"\"\n",
        "        Mengambil data untuk beberapa ticker dan beberapa interval.\n",
        "\n",
        "        Args:\n",
        "            tickers (List[str]): List simbol saham\n",
        "            intervals (List[str]): List interval data\n",
        "            row_limit (int): Maksimal jumlah baris data\n",
        "            use_threads (bool): Gunakan multithreading untuk pengambilan data\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, Dict[str, Optional[str]]]: Nested dictionary dengan struktur {ticker: {interval: filepath}}\n",
        "        \"\"\"\n",
        "        logger.info(f\"Memulai pengambilan data untuk {len(tickers)} ticker dengan {len(intervals)} timeframe\")\n",
        "        results = {}\n",
        "\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        if use_threads:\n",
        "            # Menggunakan ThreadPoolExecutor untuk menjalankan secara paralel\n",
        "            with ThreadPoolExecutor(max_workers=min(10, len(tickers))) as executor:\n",
        "                future_to_ticker = {\n",
        "                    executor.submit(self.fetch_and_save_for_ticker, ticker, intervals, row_limit): ticker\n",
        "                    for ticker in tickers\n",
        "                }\n",
        "\n",
        "                for future in future_to_ticker:\n",
        "                    ticker = future_to_ticker[future]\n",
        "                    try:\n",
        "                        results[ticker] = future.result()\n",
        "                    except Exception as ex:\n",
        "                        logger.exception(f\"Error dalam thread untuk ticker {ticker}: {ex}\")\n",
        "                        results[ticker] = {interval: None for interval in intervals}\n",
        "        else:\n",
        "            # Menggunakan pendekatan sekuensial\n",
        "            for ticker in tickers:\n",
        "                results[ticker] = self.fetch_and_save_for_ticker(ticker, intervals, row_limit)\n",
        "\n",
        "        end_time = datetime.now()\n",
        "        duration = (end_time - start_time).total_seconds()\n",
        "\n",
        "        logger.info(f\"Pengambilan data selesai dalam {duration:.2f} detik\")\n",
        "        return results\n",
        "\n",
        "    def generate_summary(self, results: Dict[str, Dict[str, Optional[str]]]) -> None:\n",
        "        \"\"\"\n",
        "        Mencetak ringkasan hasil pengambilan data.\n",
        "\n",
        "        Args:\n",
        "            results (Dict[str, Dict[str, Optional[str]]]): Hasil pengambilan data\n",
        "        \"\"\"\n",
        "        success_count = 0\n",
        "        failed_count = 0\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"RINGKASAN PENGAMBILAN DATA SAHAM\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        for ticker, intervals in results.items():\n",
        "            print(f\"\\nTicker: {ticker}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "            for interval, file_path in intervals.items():\n",
        "                if file_path:\n",
        "                    status = \"BERHASIL\"\n",
        "                    success_count += 1\n",
        "                else:\n",
        "                    status = \"GAGAL\"\n",
        "                    failed_count += 1\n",
        "\n",
        "                print(f\"  {interval}: {status} {file_path if file_path else ''}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"Total: {success_count + failed_count} pengambilan data\")\n",
        "        print(f\"Berhasil: {success_count}, Gagal: {failed_count}\")\n",
        "        print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "def main():\n",
        "    # Daftar saham yang akan diambil datanya\n",
        "    tickers = [\"PGEO.JK\", \"ANTM.JK\", \"PTBA.JK\", \"FCX\", \"TINS.JK\",\n",
        "               \"BBRI.JK\", \"BMRI.JK\", \"BBNI.JK\", \"INCO.JK\", \"TLKM.JK\"]\n",
        "\n",
        "    # Daftar interval/timeframe yang akan diambil\n",
        "    intervals = [\"15m\", \"30m\", \"1h\", \"1d\"]\n",
        "\n",
        "    # Jumlah baris data yang diinginkan\n",
        "    row_limit = 1000\n",
        "\n",
        "    # Direktori untuk menyimpan hasil\n",
        "    output_dir = \"stock_data\"\n",
        "\n",
        "    # Inisialisasi fetcher\n",
        "    fetcher = StockDataFetcher(output_dir=output_dir)\n",
        "\n",
        "    # Tampilkan perkiraan jumlah maksimum baris untuk setiap interval\n",
        "    print(\"\\nPERKIRAAN JUMLAH MAKSIMUM BARIS DATA PER INTERVAL:\")\n",
        "    print(\"-\" * 50)\n",
        "    for interval in intervals:\n",
        "        max_rows = fetcher.get_max_rows_for_interval(interval)\n",
        "        print(f\"Interval {interval}: {max_rows} baris\")\n",
        "\n",
        "    # Ambil dan simpan data\n",
        "    results = fetcher.fetch_multi_ticker_multi_timeframe(\n",
        "        tickers=tickers,\n",
        "        intervals=intervals,\n",
        "        row_limit=row_limit,\n",
        "        use_threads=True\n",
        "    )\n",
        "\n",
        "    # Tampilkan ringkasan hasil\n",
        "    fetcher.generate_summary(results)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "DGgFG-Evn7oO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-Processing Data"
      ],
      "metadata": {
        "id": "vNYTe0E6n8-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Direktori tempat file CSV disimpan\n",
        "data_dir = \"stock_data/\"\n",
        "\n",
        "# Mencari semua file CSV dalam direktori\n",
        "csv_files = glob.glob(os.path.join(data_dir, \"*.csv\"))\n",
        "\n",
        "# Nama kolom baru yang akan digunakan\n",
        "new_columns = ['Datetime', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits', 'Ticker']\n",
        "\n",
        "# Loop melalui semua file CSV\n",
        "for file_path in csv_files:\n",
        "    try:\n",
        "        # Membaca file CSV\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Mengekstrak ticker dari nama file\n",
        "        file_name = os.path.basename(file_path)\n",
        "        ticker = file_name.split('_')[0]\n",
        "        if 'JK' in file_name:\n",
        "            ticker = ticker + '.JK'\n",
        "\n",
        "        # Memeriksa jumlah kolom untuk memastikan data konsisten\n",
        "        if len(df.columns) >= 6:  # Minimal memiliki OHLCV + minimal 1 kolom lain\n",
        "            # Mengganti nama kolom\n",
        "            df.columns = new_columns[:len(df.columns)]\n",
        "\n",
        "            # Menambahkan kolom Ticker jika belum ada\n",
        "            if 'Ticker' not in df.columns:\n",
        "                df['Ticker'] = ticker\n",
        "\n",
        "            # Mengkonversi tipe data dengan penanganan timezone\n",
        "            # Menambahkan parameter utc=True untuk mengatasi warning\n",
        "            df['Datetime'] = pd.to_datetime(df['Datetime'], utc=True)\n",
        "\n",
        "            # Alternatif jika ingin zona waktu lokal:\n",
        "            # df['Datetime'] = pd.to_datetime(df['Datetime']).dt.tz_localize(None)\n",
        "\n",
        "            df['Open'] = df['Open'].astype(float)\n",
        "            df['High'] = df['High'].astype(float)\n",
        "            df['Low'] = df['Low'].astype(float)\n",
        "            df['Close'] = df['Close'].astype(float)\n",
        "\n",
        "            # Menghapus kolom Dividends dan Stock Splits jika ada\n",
        "            columns_to_keep = [col for col in df.columns if col not in ['Dividends', 'Stock Splits']]\n",
        "            df = df[columns_to_keep]\n",
        "\n",
        "            # Menyimpan kembali file CSV\n",
        "            df.to_csv(file_path, index=False)\n",
        "            print(f\"Preprocessing berhasil: {file_path}\")\n",
        "        else:\n",
        "            print(f\"Format tidak sesuai: {file_path} (jumlah kolom: {len(df.columns)})\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error memproses {file_path}: {str(e)}\")\n",
        "\n",
        "print(\"\\nPreprocessing selesai untuk semua file!\")"
      ],
      "metadata": {
        "id": "-Qhh4-AeoGOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Push to Supabase"
      ],
      "metadata": {
        "id": "6BkrBsz_oG1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from supabase import create_client, Client\n",
        "from datetime import datetime\n",
        "import time\n",
        "import glob\n",
        "from google.colab import userdata\n",
        "\n",
        "# Supabase credentials\n",
        "supabase_url = userdata.get('SUPABASE_URL')\n",
        "supabase_key = userdata.get('SUPABASE_KEY')\n",
        "\n",
        "# Initialize Supabase client\n",
        "supabase: Client = create_client(supabase_url, supabase_key)\n",
        "\n",
        "print(\"Supabase client initialized successfully!\")\n",
        "\n",
        "# Define tickers and timeframes based on your data\n",
        "tickers = [\n",
        "    'PGEO.JK', 'ANTM.JK', 'PTBA.JK', 'FCX', 'TINS.JK',\n",
        "    'BBRI.JK', 'BMRI.JK', 'BBNI.JK', 'INCO.JK', 'TLKM.JK'\n",
        "]\n",
        "timeframes = ['15m', '30m', '1h', '1d']"
      ],
      "metadata": {
        "id": "qYEMldFVoONw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_stock_csv(file_path):\n",
        "    \"\"\"Read a stock CSV file and return a pandas DataFrame.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        if 'Datetime' in df.columns:\n",
        "            df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def upload_dataframe_to_supabase(df, ticker, timeframe, table_name='stock_prices'):\n",
        "    \"\"\"Upload a DataFrame to Supabase table in chunks.\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        print(f\"No data to upload for {ticker} - {timeframe}\")\n",
        "        return\n",
        "\n",
        "    df['Ticker'] = ticker\n",
        "    df['Timeframe'] = timeframe\n",
        "\n",
        "    # âœ… Serialize datetime to ISO string\n",
        "    if 'Datetime' in df.columns:\n",
        "        df['Datetime'] = df['Datetime'].dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
        "\n",
        "    records = df.to_dict('records')\n",
        "    chunk_size = 1000\n",
        "    chunks = [records[i:i + chunk_size] for i in range(0, len(records), chunk_size)]\n",
        "\n",
        "    total_uploaded = 0\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        try:\n",
        "            response = supabase.table(table_name).insert(chunk).execute()\n",
        "\n",
        "            if hasattr(response, 'error') and response.error:\n",
        "                print(f\"Error uploading chunk {i+1} for {ticker} - {timeframe}: {response.error}\")\n",
        "            else:\n",
        "                chunk_uploaded = len(chunk)\n",
        "                total_uploaded += chunk_uploaded\n",
        "                print(f\"âœ… Uploaded {chunk_uploaded} records for {ticker} - {timeframe} (chunk {i+1}/{len(chunks)})\")\n",
        "\n",
        "            time.sleep(0.1)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Exception uploading chunk {i+1} for {ticker} - {timeframe}: {e}\")\n",
        "\n",
        "    print(f\"âœ… Total uploaded for {ticker} - {timeframe}: {total_uploaded}\")\n",
        "    return total_uploaded\n",
        "\n",
        "def upload_all_stock_data_auto(base_dir='stock_data'):\n",
        "    \"\"\"Automatically find and upload all stock data CSV files.\"\"\"\n",
        "    total_records = 0\n",
        "    csv_files = glob.glob(f\"{base_dir}/*.csv\")\n",
        "\n",
        "    for file_path in csv_files:\n",
        "        filename = os.path.basename(file_path)\n",
        "        if '_' not in filename or not filename.endswith('.csv'):\n",
        "            print(f\"Skipping file: {filename}\")\n",
        "            continue\n",
        "\n",
        "        parts = filename.replace('.csv', '').split('_')\n",
        "        if len(parts) < 2:\n",
        "            print(f\"Skipping file with unexpected format: {filename}\")\n",
        "            continue\n",
        "\n",
        "        timeframe = parts[-1]\n",
        "        ticker_parts = parts[:-1]\n",
        "\n",
        "        # Reconstruct ticker\n",
        "        if len(ticker_parts) == 2 and ticker_parts[1] == 'JK':\n",
        "            ticker = f\"{ticker_parts[0]}.{ticker_parts[1]}\"\n",
        "        else:\n",
        "            ticker = '_'.join(ticker_parts)\n",
        "\n",
        "        print(f\"\\nðŸ”„ Processing {ticker} - {timeframe} from {file_path}\")\n",
        "\n",
        "        df = read_stock_csv(file_path)\n",
        "        records_uploaded = upload_dataframe_to_supabase(df, ticker, timeframe)\n",
        "        if records_uploaded:\n",
        "            total_records += records_uploaded\n",
        "\n",
        "    print(f\"\\nâœ… Total all records uploaded to Supabase: {total_records}\")\n",
        "    return total_records"
      ],
      "metadata": {
        "id": "Ysuxpgr7oPFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# company_profitabilities"
      ],
      "metadata": {
        "id": "edAUANbX4W6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping"
      ],
      "metadata": {
        "id": "uO5JDuNt4iu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import logging\n",
        "import os\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from typing import List, Dict\n",
        "\n",
        "# Konfigurasi logging\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class CompanyInfoFetcher:\n",
        "    \"\"\"\n",
        "    Fetcher untuk mengambil data return on equity dan return on assets dari Yahoo Finance.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_dir: str = \"company_data\"):\n",
        "        self.output_dir = output_dir\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "            logger.info(f\"Output directory created: {output_dir}\")\n",
        "\n",
        "    def fetch_company_info(self, ticker: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Ambil data ticker, returnOnEquity, returnOnAssets.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logger.info(f\"Fetching company profitability for {ticker}\")\n",
        "            stock = yf.Ticker(ticker)\n",
        "            info = stock.info\n",
        "\n",
        "            return {\n",
        "                'ticker': ticker,\n",
        "                'returnOnEquity': info.get('returnOnEquity', None),\n",
        "                'returnOnAssets': info.get('returnOnAssets', None)\n",
        "            }\n",
        "\n",
        "        except Exception as ex:\n",
        "            logger.exception(f\"Failed to fetch data for {ticker}: {ex}\")\n",
        "            return {\n",
        "                'ticker': ticker,\n",
        "                'returnOnEquity': None,\n",
        "                'returnOnAssets': None\n",
        "            }\n",
        "\n",
        "    def fetch_for_ticker(self, ticker: str) -> Dict:\n",
        "        return self.fetch_company_info(ticker)\n",
        "\n",
        "    def fetch_multi_tickers(self, tickers: List[str], use_threads: bool = True) -> pd.DataFrame:\n",
        "        logger.info(f\"Fetching data for {len(tickers)} tickers...\")\n",
        "        data_list = []\n",
        "\n",
        "        if use_threads:\n",
        "            with ThreadPoolExecutor(max_workers=min(10, len(tickers))) as executor:\n",
        "                futures = {executor.submit(self.fetch_for_ticker, ticker): ticker for ticker in tickers}\n",
        "                for future in futures:\n",
        "                    try:\n",
        "                        data = future.result()\n",
        "                        data_list.append(data)\n",
        "                    except Exception as ex:\n",
        "                        logger.exception(f\"Thread error: {ex}\")\n",
        "        else:\n",
        "            for ticker in tickers:\n",
        "                data = self.fetch_for_ticker(ticker)\n",
        "                data_list.append(data)\n",
        "\n",
        "        df = pd.DataFrame(data_list)\n",
        "        return df\n",
        "\n",
        "def main():\n",
        "    tickers = [\n",
        "        \"PGEO.JK\", \"ANTM.JK\", \"PTBA.JK\", \"FCX\", \"TINS.JK\",\n",
        "        \"BBRI.JK\", \"BMRI.JK\", \"BBNI.JK\", \"INCO.JK\", \"TLKM.JK\"\n",
        "    ]\n",
        "\n",
        "    output_dir = \"company_data\"\n",
        "    fetcher = CompanyInfoFetcher(output_dir=output_dir)\n",
        "\n",
        "    df = fetcher.fetch_multi_tickers(tickers, use_threads=True)\n",
        "\n",
        "    output_path = os.path.join(output_dir, \"company_profitabilities.csv\")\n",
        "    df.to_csv(output_path, index=False)\n",
        "\n",
        "    logger.info(f\"âœ… CSV saved at: {output_path}\")\n",
        "    print(f\"\\nðŸŽ‰ DONE! File saved as: {output_path}\")\n",
        "    print(df.head())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HE75a6Dy4qWD",
        "outputId": "50d2281d-e762-47f5-dd7f-16c93be8f385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸŽ‰ DONE! File saved as: company_data/company_profitabilities.csv\n",
            "    ticker  returnOnEquity  returnOnAssets\n",
            "0  PGEO.JK         0.08055         0.04367\n",
            "1  ANTM.JK             NaN             NaN\n",
            "2  PTBA.JK         0.23252         0.08583\n",
            "3      FCX         0.15686         0.08082\n",
            "4  TINS.JK         0.17334         0.08412\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Push to Supabase"
      ],
      "metadata": {
        "id": "SqkvtIwa4kEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from supabase import create_client, Client\n",
        "from google.colab import userdata\n",
        "\n",
        "def upload_to_supabase():\n",
        "    supabase_url = userdata.get('SUPABASE_URL')\n",
        "    supabase_key = userdata.get('SUPABASE_KEY')\n",
        "    supabase: Client = create_client(supabase_url, supabase_key)\n",
        "    print(\"Supabase client initialized successfully!\")\n",
        "\n",
        "    df = pd.read_csv(\"/content/company_data/company_profitabilities.csv\")\n",
        "    df.columns = [col.lower() for col in df.columns]\n",
        "\n",
        "    # Optional: clear existing data\n",
        "    supabase.table(\"company_profitabilities\").delete().neq('ticker', '').execute()\n",
        "    print(\"ðŸ§¹ Old data cleared\")\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        data = {\n",
        "            \"ticker\": row['ticker'],\n",
        "            \"returnonequity\": row['returnonequity'],\n",
        "            \"returnonassets\": row['returnonassets']\n",
        "        }\n",
        "        supabase.table(\"company_profitabilities\").insert(data).execute()\n",
        "        time.sleep(0.1)\n",
        "\n",
        "    print(\"ðŸš€ All data uploaded to Supabase!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    upload_to_supabase()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Doek4tBs4WUc",
        "outputId": "b127d9b0-16f2-445a-99f4-96f137e78cb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Supabase client initialized successfully!\n",
            "ðŸ§¹ Old data cleared\n",
            "ðŸš€ All data uploaded to Supabase!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# company_dividend"
      ],
      "metadata": {
        "id": "IKLMFL9K9B0q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping"
      ],
      "metadata": {
        "id": "SdGnCmrN9OZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import logging\n",
        "import os\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from typing import List, Dict\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class CompanyDividendFetcher:\n",
        "    \"\"\"\n",
        "    Fetcher untuk mengambil data dividend dari Yahoo Finance.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_dir: str = \"company_data\"):\n",
        "        self.output_dir = output_dir\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "            logger.info(f\"ðŸ“ Output directory created: {output_dir}\")\n",
        "\n",
        "    def fetch_company_dividend(self, ticker: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Ambil data dividend info: dividendrate, dividendyield, exdividenddate, payoutratio.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logger.info(f\"ðŸ“¥ Fetching dividend data for {ticker}\")\n",
        "            stock = yf.Ticker(ticker)\n",
        "            info = stock.info\n",
        "\n",
        "            return {\n",
        "                'ticker': ticker,\n",
        "                'dividendrate': info.get('dividendRate', None),\n",
        "                'dividendyield': info.get('dividendYield', None),\n",
        "                'exdividenddate': info.get('exDividendDate', None),\n",
        "                'payoutratio': info.get('payoutRatio', None)\n",
        "            }\n",
        "\n",
        "        except Exception as ex:\n",
        "            logger.exception(f\"âŒ Failed to fetch data for {ticker}: {ex}\")\n",
        "            return {\n",
        "                'ticker': ticker,\n",
        "                'dividendrate': None,\n",
        "                'dividendyield': None,\n",
        "                'exdividenddate': None,\n",
        "                'payoutratio': None\n",
        "            }\n",
        "\n",
        "    def fetch_multi_tickers(self, tickers: List[str], use_threads: bool = True) -> pd.DataFrame:\n",
        "        logger.info(f\"ðŸš€ Fetching dividend data for {len(tickers)} tickers...\")\n",
        "        data_list = []\n",
        "\n",
        "        if use_threads:\n",
        "            with ThreadPoolExecutor(max_workers=min(10, len(tickers))) as executor:\n",
        "                futures = {executor.submit(self.fetch_company_dividend, ticker): ticker for ticker in tickers}\n",
        "                for future in futures:\n",
        "                    try:\n",
        "                        data = future.result()\n",
        "                        data_list.append(data)\n",
        "                    except Exception as ex:\n",
        "                        logger.exception(f\"Thread error: {ex}\")\n",
        "        else:\n",
        "            for ticker in tickers:\n",
        "                data = self.fetch_company_dividend(ticker)\n",
        "                data_list.append(data)\n",
        "\n",
        "        df = pd.DataFrame(data_list)\n",
        "        return df\n",
        "\n",
        "def main():\n",
        "    tickers = [\n",
        "        \"PGEO.JK\", \"ANTM.JK\", \"PTBA.JK\", \"FCX\", \"TINS.JK\",\n",
        "        \"BBRI.JK\", \"BMRI.JK\", \"BBNI.JK\", \"INCO.JK\", \"TLKM.JK\"\n",
        "    ]\n",
        "\n",
        "    output_dir = \"company_data\"\n",
        "    fetcher = CompanyDividendFetcher(output_dir=output_dir)\n",
        "\n",
        "    df = fetcher.fetch_multi_tickers(tickers, use_threads=True)\n",
        "\n",
        "    # Format exdividenddate ke tanggal (kalau bentuknya epoch timestamp)\n",
        "    if df[\"exdividenddate\"].notnull().any():\n",
        "        df[\"exdividenddate\"] = pd.to_datetime(df[\"exdividenddate\"], unit='s', errors='coerce').dt.date\n",
        "\n",
        "    output_path = os.path.join(output_dir, \"company_dividend.csv\")\n",
        "    df.to_csv(output_path, index=False)\n",
        "\n",
        "    logger.info(f\"âœ… CSV saved at: {output_path}\")\n",
        "    print(f\"\\nðŸŽ‰ DONE! File saved as: {output_path}\")\n",
        "    print(df.head())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpX9AiCJ9ZL6",
        "outputId": "22206080-0392-4478-c489-72d8cb9a1f02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸŽ‰ DONE! File saved as: company_data/company_dividend.csv\n",
            "    ticker  dividendrate  dividendyield exdividenddate  payoutratio\n",
            "0  PGEO.JK         47.77           5.69     2024-06-06       0.7739\n",
            "1  ANTM.JK        128.07           6.58     2024-05-21       1.2664\n",
            "2  PTBA.JK        397.71          14.57     2024-05-21       0.8957\n",
            "3      FCX          0.60           1.82     2025-04-15       0.4615\n",
            "4  TINS.JK           NaN            NaN     2023-06-26       0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Push to Supabase"
      ],
      "metadata": {
        "id": "XRwEob_i9Trd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from supabase import create_client, Client\n",
        "from google.colab import userdata\n",
        "import math\n",
        "\n",
        "def safe_float(val, max_abs_value=1e6):  # default max 1 juta, bisa override\n",
        "    try:\n",
        "        if pd.isna(val) or val in ['NaT', 'nan'] or (isinstance(val, float) and math.isinf(val)):\n",
        "            return None\n",
        "        val = float(val)\n",
        "        if abs(val) > max_abs_value:\n",
        "            return None\n",
        "        return round(val, 6)  # cukup aman buat 99% kolom\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def upload_to_supabase():\n",
        "    supabase_url = userdata.get('SUPABASE_URL')\n",
        "    supabase_key = userdata.get('SUPABASE_KEY')\n",
        "    supabase: Client = create_client(supabase_url, supabase_key)\n",
        "    print(\"âœ… Supabase client initialized successfully!\")\n",
        "\n",
        "    df = pd.read_csv(\"/content/company_data/company_dividend.csv\")\n",
        "    df.columns = [col.lower() for col in df.columns]\n",
        "\n",
        "    # Bersihin data sebelumnya\n",
        "    supabase.table(\"company_dividend\").delete().neq('ticker', '').execute()\n",
        "    print(\"ðŸ§¹ Old data in `company_dividend` cleared\")\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        data = {\n",
        "            \"ticker\": row['ticker'],\n",
        "            \"dividendrate\": safe_float(row['dividendrate'], max_abs_value=9.9999),\n",
        "            \"dividendyield\": safe_float(row['dividendyield']),\n",
        "            \"exdividenddate\": row['exdividenddate'],  # string, tetap kirim as is\n",
        "            \"payoutratio\": safe_float(row['payoutratio']),\n",
        "        }\n",
        "        supabase.table(\"company_dividend\").insert(data).execute()\n",
        "        time.sleep(0.1)\n",
        "\n",
        "    print(\"ðŸš€ Dividend data uploaded to Supabase!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()  # make sure this generates the CSV before upload\n",
        "    upload_to_supabase()\n"
      ],
      "metadata": {
        "id": "0tA-lK_q9aJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# company_growth"
      ],
      "metadata": {
        "id": "z6R8RumgDM9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping"
      ],
      "metadata": {
        "id": "Px8d-IEmDP6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import logging\n",
        "import os\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from typing import List, Dict\n",
        "\n",
        "# Konfigurasi logging\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class CompanyInfoFetcher:\n",
        "    \"\"\"\n",
        "    Fetcher untuk mengambil data pertumbuhan perusahaan dari Yahoo Finance.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_dir: str = \"company_data\"):\n",
        "        self.output_dir = output_dir\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "            logger.info(f\"Output directory created: {output_dir}\")\n",
        "\n",
        "    def fetch_company_growth(self, ticker: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Ambil data ticker, revenueGrowth, earningsGrowth, earningsQuarterlyGrowth.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logger.info(f\"Fetching company growth for {ticker}\")\n",
        "            stock = yf.Ticker(ticker)\n",
        "            info = stock.info\n",
        "\n",
        "            return {\n",
        "                'ticker': ticker,\n",
        "                'revenuegrowth': info.get('revenueGrowth', None),\n",
        "                'earningsgrowth': info.get('earningsGrowth', None),\n",
        "                'earningsquarterlygrowth': info.get('earningsQuarterlyGrowth', None)\n",
        "            }\n",
        "\n",
        "        except Exception as ex:\n",
        "            logger.exception(f\"Failed to fetch data for {ticker}: {ex}\")\n",
        "            return {\n",
        "                'ticker': ticker,\n",
        "                'revenuegrowth': None,\n",
        "                'earningsgrowth': None,\n",
        "                'earningsquarterlygrowth': None\n",
        "            }\n",
        "\n",
        "    def fetch_for_ticker(self, ticker: str) -> Dict:\n",
        "        return self.fetch_company_growth(ticker)\n",
        "\n",
        "    def fetch_multi_tickers(self, tickers: List[str], use_threads: bool = True) -> pd.DataFrame:\n",
        "        logger.info(f\"Fetching data for {len(tickers)} tickers...\")\n",
        "        data_list = []\n",
        "\n",
        "        if use_threads:\n",
        "            with ThreadPoolExecutor(max_workers=min(10, len(tickers))) as executor:\n",
        "                futures = {executor.submit(self.fetch_for_ticker, ticker): ticker for ticker in tickers}\n",
        "                for future in futures:\n",
        "                    try:\n",
        "                        data = future.result()\n",
        "                        data_list.append(data)\n",
        "                    except Exception as ex:\n",
        "                        logger.exception(f\"Thread error: {ex}\")\n",
        "        else:\n",
        "            for ticker in tickers:\n",
        "                data = self.fetch_for_ticker(ticker)\n",
        "                data_list.append(data)\n",
        "\n",
        "        df = pd.DataFrame(data_list)\n",
        "        return df\n",
        "\n",
        "def main():\n",
        "    tickers = [\n",
        "        \"PGEO.JK\", \"ANTM.JK\", \"PTBA.JK\", \"FCX\", \"TINS.JK\",\n",
        "        \"BBRI.JK\", \"BMRI.JK\", \"BBNI.JK\", \"INCO.JK\", \"TLKM.JK\"\n",
        "    ]\n",
        "\n",
        "    output_dir = \"company_data\"\n",
        "    fetcher = CompanyInfoFetcher(output_dir=output_dir)\n",
        "\n",
        "    df = fetcher.fetch_multi_tickers(tickers, use_threads=True)\n",
        "\n",
        "    output_path = os.path.join(output_dir, \"company_growth.csv\")\n",
        "    df.to_csv(output_path, index=False)\n",
        "\n",
        "    logger.info(f\"âœ… CSV saved at: {output_path}\")\n",
        "    print(f\"\\nðŸŽ‰ DONE! File saved as: {output_path}\")\n",
        "    print(df.head())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVubOjVXDRsn",
        "outputId": "c1439b75-e24b-41ee-f848-7c4e8c838191"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸŽ‰ DONE! File saved as: company_data/company_growth.csv\n",
            "    ticker  revenuegrowth  earningsgrowth  earningsquarterlygrowth\n",
            "0  PGEO.JK          0.031          -0.143                   -0.119\n",
            "1  ANTM.JK          1.561           5.313                    5.313\n",
            "2  PTBA.JK          0.126          -0.198                   -0.195\n",
            "3      FCX         -0.031          -0.292                   -0.294\n",
            "4  TINS.JK          0.291             NaN                      NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Push to Supabase"
      ],
      "metadata": {
        "id": "SpGh41x_DbuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from supabase import create_client, Client\n",
        "from google.colab import userdata\n",
        "import math\n",
        "\n",
        "def upload_to_supabase():\n",
        "    supabase_url = userdata.get('SUPABASE_URL')\n",
        "    supabase_key = userdata.get('SUPABASE_KEY')\n",
        "    supabase: Client = create_client(supabase_url, supabase_key)\n",
        "    print(\"âœ… Supabase client initialized successfully!\")\n",
        "\n",
        "    df = pd.read_csv(\"/content/company_data/company_growth.csv\")\n",
        "    df.columns = [col.lower() for col in df.columns]\n",
        "\n",
        "    # Optional: clear existing data\n",
        "    supabase.table(\"company_growth\").delete().neq('ticker', '').execute()\n",
        "    print(\"ðŸ§¹ Old data cleared\")\n",
        "\n",
        "    # NaN dan float yang gak valid akan diubah jadi None\n",
        "    def safe_float(val):\n",
        "        if pd.isna(val) or math.isinf(val):\n",
        "            return None\n",
        "        return float(val)\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        data = {\n",
        "            \"ticker\": row['ticker'],\n",
        "            \"revenuegrowth\": safe_float(row['revenuegrowth']),\n",
        "            \"earningsgrowth\": safe_float(row['earningsgrowth']),\n",
        "            \"earningsquarterlygrowth\": safe_float(row['earningsquarterlygrowth'])\n",
        "        }\n",
        "        supabase.table(\"company_growth\").insert(data).execute()\n",
        "        time.sleep(0.1)\n",
        "\n",
        "    print(\"ðŸš€ All data uploaded to Supabase!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    upload_to_supabase()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qR9HyrmUDeX5",
        "outputId": "9da6f4ce-0eb5-4aaf-9734-4080619de37e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸŽ‰ DONE! File saved as: company_data/company_growth.csv\n",
            "    ticker  revenuegrowth  earningsgrowth  earningsquarterlygrowth\n",
            "0  PGEO.JK          0.031          -0.143                   -0.119\n",
            "1  ANTM.JK          1.561           5.313                    5.313\n",
            "2  PTBA.JK          0.126          -0.198                   -0.195\n",
            "3      FCX         -0.031          -0.292                   -0.294\n",
            "4  TINS.JK          0.291             NaN                      NaN\n",
            "âœ… Supabase client initialized successfully!\n",
            "ðŸ§¹ Old data cleared\n",
            "ðŸš€ All data uploaded to Supabase!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nfYoCer3PiGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# company_finance"
      ],
      "metadata": {
        "id": "4UKAKYQPCnX-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping Data"
      ],
      "metadata": {
        "id": "Q7InfyhPCuqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import logging\n",
        "import os\n",
        "import json\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class CompanyInfoFetcher:\n",
        "    def __init__(self, output_dir: str = \"company_data\"):\n",
        "        self.output_dir = output_dir\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "            logger.info(f\"ðŸ“ Output directory created: {output_dir}\")\n",
        "\n",
        "    def fetch_company_finance(self, ticker: str) -> Dict:\n",
        "        try:\n",
        "            logger.info(f\"ðŸ“Š Fetching financial info for {ticker}\")\n",
        "            stock = yf.Ticker(ticker)\n",
        "            info = stock.info\n",
        "\n",
        "            financial_data = {\n",
        "                'ticker': ticker.upper(),  # Uppercase ticker\n",
        "                'marketcap': info.get('marketCap'),\n",
        "                'shareoutstanding': info.get('sharesOutstanding'),\n",
        "                'totalrevenue': info.get('totalRevenue'),\n",
        "                'netincometocommon': info.get('netIncomeToCommon'),\n",
        "                'profitmargins': info.get('profitMargins'),\n",
        "                'trailingeps': info.get('trailingEps'),\n",
        "                'forwardeps': info.get('forwardEps'),\n",
        "                'grossmargins': info.get('grossMargins'),\n",
        "                'operatingmargins': info.get('operatingMargins'),\n",
        "                'operatingcashflow': info.get('operatingCashflow'),\n",
        "                'freecashflow': info.get('freeCashflow')\n",
        "            }\n",
        "\n",
        "            logger.info(f\"âœ… Success for {ticker}\")\n",
        "            return financial_data\n",
        "\n",
        "        except Exception as ex:\n",
        "            logger.exception(f\"âŒ Failed to fetch for {ticker}: {ex}\")\n",
        "            return {\n",
        "                'ticker': ticker.upper(),\n",
        "                'error': str(ex)\n",
        "            }\n",
        "\n",
        "    def fetch_multi_tickers(self, tickers: List[str], use_threads: bool = True) -> List[Dict]:\n",
        "        logger.info(f\"ðŸš€ Starting to fetch {len(tickers)} tickers\")\n",
        "        results = []\n",
        "\n",
        "        if use_threads:\n",
        "            with ThreadPoolExecutor(max_workers=min(10, len(tickers))) as executor:\n",
        "                futures = {\n",
        "                    executor.submit(self.fetch_company_finance, ticker): ticker\n",
        "                    for ticker in tickers\n",
        "                }\n",
        "                for future in futures:\n",
        "                    ticker = futures[future]\n",
        "                    try:\n",
        "                        result = future.result()\n",
        "                        results.append(result)\n",
        "                    except Exception as ex:\n",
        "                        logger.exception(f\"Thread error for {ticker}: {ex}\")\n",
        "                        results.append({\n",
        "                            'ticker': ticker.upper(),\n",
        "                            'error': str(ex)\n",
        "                        })\n",
        "        else:\n",
        "            for ticker in tickers:\n",
        "                results.append(self.fetch_company_finance(ticker))\n",
        "\n",
        "        logger.info(\"âœ… All data fetched\")\n",
        "        return results\n",
        "\n",
        "    def save_all_to_csv(self, data: List[Dict]) -> Optional[str]:\n",
        "        if not data:\n",
        "            logger.warning(\"âš ï¸ No data to save.\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            df = pd.DataFrame(data)\n",
        "            filepath = os.path.join(self.output_dir, \"all_financial_data.csv\")\n",
        "            df.to_csv(filepath, index=False)\n",
        "\n",
        "            json_path = os.path.join(self.output_dir, \"all_financial_data.json\")\n",
        "            with open(json_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "            logger.info(f\"ðŸ’¾ Saved CSV to {filepath}\")\n",
        "            logger.info(f\"ðŸ’¾ Saved JSON to {json_path}\")\n",
        "            return filepath\n",
        "        except Exception as ex:\n",
        "            logger.exception(f\"âŒ Failed to save data: {ex}\")\n",
        "            return None\n",
        "\n",
        "    def generate_summary(self, results: List[Dict]) -> None:\n",
        "        success_count = sum(1 for r in results if 'error' not in r)\n",
        "        failed_count = len(results) - success_count\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"ðŸ“‹ FINANCIAL DATA FETCHING SUMMARY\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        for result in results:\n",
        "            status = \"âœ… SUCCESS\" if 'error' not in result else \"âŒ FAILED\"\n",
        "            print(f\"{result['ticker']}: {status}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(f\"Total tickers: {len(results)}\")\n",
        "        print(f\"âœ… Success: {success_count}\")\n",
        "        print(f\"âŒ Failed: {failed_count}\")\n",
        "        print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    tickers = [\n",
        "        \"PGEO.JK\", \"ANTM.JK\", \"PTBA.JK\", \"FCX\", \"TINS.JK\",\n",
        "        \"BBRI.JK\", \"BMRI.JK\", \"BBNI.JK\", \"INCO.JK\", \"TLKM.JK\"\n",
        "    ]\n",
        "    output_dir = \"company_data\"\n",
        "\n",
        "    fetcher = CompanyInfoFetcher(output_dir=output_dir)\n",
        "    results = fetcher.fetch_multi_tickers(tickers, use_threads=True)\n",
        "    fetcher.save_all_to_csv(results)\n",
        "    fetcher.generate_summary(results)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UerNM14oPo0k",
        "outputId": "11912f71-84e1-4bb9-bf2e-244d36bdfc65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ðŸ“‹ FINANCIAL DATA FETCHING SUMMARY\n",
            "================================================================================\n",
            "PGEO.JK: âœ… SUCCESS\n",
            "ANTM.JK: âœ… SUCCESS\n",
            "PTBA.JK: âœ… SUCCESS\n",
            "FCX: âœ… SUCCESS\n",
            "TINS.JK: âœ… SUCCESS\n",
            "BBRI.JK: âœ… SUCCESS\n",
            "BMRI.JK: âœ… SUCCESS\n",
            "BBNI.JK: âœ… SUCCESS\n",
            "INCO.JK: âœ… SUCCESS\n",
            "TLKM.JK: âœ… SUCCESS\n",
            "\n",
            "================================================================================\n",
            "Total tickers: 10\n",
            "âœ… Success: 10\n",
            "âŒ Failed: 0\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Push to Supabase"
      ],
      "metadata": {
        "id": "G1QZkMC_E_ZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload_to_supabase.py\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "import math\n",
        "from supabase import create_client, Client\n",
        "from google.colab import userdata\n",
        "\n",
        "def upload_to_supabase():\n",
        "    supabase_url = userdata.get('SUPABASE_URL')\n",
        "    supabase_key = userdata.get('SUPABASE_KEY')\n",
        "    supabase: Client = create_client(supabase_url, supabase_key)\n",
        "    print(\"âœ… Supabase client initialized successfully!\")\n",
        "\n",
        "    df = pd.read_csv(\"company_data/all_financial_data.csv\")\n",
        "    df.columns = [col.lower() for col in df.columns]\n",
        "\n",
        "    # Optional: clear old data\n",
        "    supabase.table(\"company_finance\").delete().neq('ticker', '').execute()\n",
        "    print(\"ðŸ§¹ Old data cleared\")\n",
        "\n",
        "    def safe_float(val):\n",
        "        if pd.isna(val) or math.isinf(val):\n",
        "            return None\n",
        "        return float(val)\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        data = {\n",
        "            \"ticker\": row.get('ticker'),\n",
        "            \"marketcap\": safe_float(row.get('marketcap')),\n",
        "            \"shareoutstanding\": safe_float(row.get('shareoutstanding')),\n",
        "            \"totalrevenue\": safe_float(row.get('totalrevenue')),\n",
        "            \"netincometocommon\": safe_float(row.get('netincometocommon')),\n",
        "            \"profitmargins\": safe_float(row.get('profitmargins')),\n",
        "            \"trailingeps\": safe_float(row.get('trailingeps')),\n",
        "            \"forwardeps\": safe_float(row.get('forwardeps')),\n",
        "            \"grossmargins\": safe_float(row.get('grossmargins')),\n",
        "            \"operatingmargins\": safe_float(row.get('operatingmargins')),\n",
        "            \"operatingcashflow\": safe_float(row.get('operatingcashflow')),\n",
        "            \"freecashflow\": safe_float(row.get('freecashflow'))\n",
        "        }\n",
        "        supabase.table(\"company_finance\").insert(data).execute()\n",
        "        time.sleep(0.1)\n",
        "\n",
        "    print(\"ðŸš€ Data uploaded to Supabase!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    upload_to_supabase()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W14KUkraPyMU",
        "outputId": "2f4ce09d-abc2-4102-ede7-0377aafba8dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Supabase client initialized successfully!\n",
            "ðŸ§¹ Old data cleared\n",
            "ðŸš€ Data uploaded to Supabase!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# company_valuation"
      ],
      "metadata": {
        "id": "sL42goykP-3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping Data"
      ],
      "metadata": {
        "id": "2wa7UQSEQEeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import logging\n",
        "import os\n",
        "import json\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "# Logging setup\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class CompanyValuationFetcher:\n",
        "    def __init__(self, output_dir: str = \"company_data\"):\n",
        "        self.output_dir = output_dir\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "            logger.info(f\"Output directory created: {output_dir}\")\n",
        "\n",
        "    def fetch_company_valuation(self, ticker: str) -> Dict:\n",
        "        try:\n",
        "            logger.info(f\"Fetching valuation info for {ticker}\")\n",
        "            stock = yf.Ticker(ticker)\n",
        "            info = stock.info\n",
        "\n",
        "            valuation_data = {\n",
        "                'ticker': ticker.upper(),\n",
        "                'trailingpe': info.get('trailingPE'),\n",
        "                'forwardpe': info.get('forwardPE'),\n",
        "                'pegratio': info.get('pegRatio'),\n",
        "                'pricetobook': info.get('priceToBook'),\n",
        "                'pricetosalestrailing12months': info.get('priceToSalesTrailing12Months')\n",
        "            }\n",
        "\n",
        "            logger.info(f\"Successfully fetched valuation info for {ticker}\")\n",
        "            return valuation_data\n",
        "\n",
        "        except Exception as ex:\n",
        "            logger.exception(f\"Failed to fetch valuation info for {ticker}: {ex}\")\n",
        "            return {\n",
        "                'ticker': ticker.upper(),\n",
        "                'error': str(ex)\n",
        "            }\n",
        "\n",
        "    def fetch_multi_tickers(self, tickers: List[str], use_threads: bool = True) -> List[Dict]:\n",
        "        logger.info(f\"Starting data fetching for {len(tickers)} tickers\")\n",
        "        results = []\n",
        "\n",
        "        if use_threads:\n",
        "            with ThreadPoolExecutor(max_workers=min(10, len(tickers))) as executor:\n",
        "                futures = {\n",
        "                    executor.submit(self.fetch_company_valuation, ticker): ticker\n",
        "                    for ticker in tickers\n",
        "                }\n",
        "                for future in futures:\n",
        "                    ticker = futures[future]\n",
        "                    try:\n",
        "                        result = future.result()\n",
        "                        results.append(result)\n",
        "                    except Exception as ex:\n",
        "                        logger.exception(f\"Error in thread for ticker {ticker}: {ex}\")\n",
        "                        results.append({\n",
        "                            'ticker': ticker.upper(),\n",
        "                            'error': str(ex)\n",
        "                        })\n",
        "        else:\n",
        "            for ticker in tickers:\n",
        "                results.append(self.fetch_company_valuation(ticker))\n",
        "\n",
        "        logger.info(\"Data fetching completed\")\n",
        "        return results\n",
        "\n",
        "    def save_all_to_csv(self, data: List[Dict]) -> Optional[str]:\n",
        "        if not data:\n",
        "            logger.warning(\"No data to save.\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            df = pd.DataFrame(data)\n",
        "            filepath = os.path.join(self.output_dir, \"company_valuation_data.csv\")\n",
        "            df.to_csv(filepath, index=False)\n",
        "\n",
        "            json_path = os.path.join(self.output_dir, \"company_valuation_data.json\")\n",
        "            with open(json_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "            logger.info(f\"All valuation data saved to {filepath}\")\n",
        "            return filepath\n",
        "        except Exception as ex:\n",
        "            logger.exception(f\"Failed to save data: {ex}\")\n",
        "            return None\n",
        "\n",
        "    def generate_summary(self, results: List[Dict]) -> None:\n",
        "        success_count = sum(1 for r in results if 'error' not in r)\n",
        "        failed_count = len(results) - success_count\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"VALUATION DATA FETCHING SUMMARY\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        for result in results:\n",
        "            status = \"SUCCESS\" if 'error' not in result else \"FAILED\"\n",
        "            print(f\"{result['ticker']}: {status}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(f\"Total: {len(results)} tickers\")\n",
        "        print(f\"Success: {success_count}, Failed: {failed_count}\")\n",
        "        print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    tickers = [\n",
        "        \"PGEO.JK\", \"ANTM.JK\", \"PTBA.JK\", \"FCX\", \"TINS.JK\",\n",
        "        \"BBRI.JK\", \"BMRI.JK\", \"BBNI.JK\", \"INCO.JK\", \"TLKM.JK\"\n",
        "    ]\n",
        "    output_dir = \"company_data\"\n",
        "\n",
        "    fetcher = CompanyValuationFetcher(output_dir=output_dir)\n",
        "    results = fetcher.fetch_multi_tickers(tickers, use_threads=True)\n",
        "    fetcher.save_all_to_csv(results)\n",
        "    fetcher.generate_summary(results)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzhTzOYOQGTA",
        "outputId": "eef3953b-9e81-45e2-a908-9633bd3007da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "VALUATION DATA FETCHING SUMMARY\n",
            "================================================================================\n",
            "PGEO.JK: SUCCESS\n",
            "ANTM.JK: SUCCESS\n",
            "PTBA.JK: SUCCESS\n",
            "FCX: SUCCESS\n",
            "TINS.JK: SUCCESS\n",
            "BBRI.JK: SUCCESS\n",
            "BMRI.JK: SUCCESS\n",
            "BBNI.JK: SUCCESS\n",
            "INCO.JK: SUCCESS\n",
            "TLKM.JK: SUCCESS\n",
            "\n",
            "================================================================================\n",
            "Total: 10 tickers\n",
            "Success: 10, Failed: 0\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Push to Supabase"
      ],
      "metadata": {
        "id": "ErVCuK1BQKC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload_to_supabase.py\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "import math\n",
        "from supabase import create_client, Client\n",
        "from google.colab import userdata\n",
        "\n",
        "def upload_to_supabase():\n",
        "    supabase_url = userdata.get('SUPABASE_URL')\n",
        "    supabase_key = userdata.get('SUPABASE_KEY')\n",
        "    supabase: Client = create_client(supabase_url, supabase_key)\n",
        "    print(\"âœ… Supabase client initialized successfully!\")\n",
        "\n",
        "    df = pd.read_csv(\"company_data/company_valuation_data.csv\")\n",
        "    df.columns = [col.lower() for col in df.columns]\n",
        "\n",
        "    # Optional: clear old data\n",
        "    supabase.table(\"company_valuation\").delete().neq('ticker', '').execute()\n",
        "    print(\"ðŸ§¹ Old data cleared\")\n",
        "\n",
        "    def safe_float(val):\n",
        "        if pd.isna(val) or math.isinf(val):\n",
        "            return None\n",
        "        return float(val)\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        data = {\n",
        "            \"ticker\": row.get('ticker'),\n",
        "            \"trailingpe\": safe_float(row.get('trailingpe')),\n",
        "            \"forwardpe\": safe_float(row.get('forwardpe')),\n",
        "            \"pegratio\": safe_float(row.get('pegratio')),\n",
        "            \"pricetobook\": safe_float(row.get('pricetobook')),\n",
        "            \"pricetosalestrailing12months\": safe_float(row.get('pricetosalestrailing12months'))\n",
        "        }\n",
        "        supabase.table(\"company_valuation\").insert(data).execute()\n",
        "        time.sleep(0.1)\n",
        "\n",
        "    print(\"ðŸš€ Data uploaded to Supabase!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    upload_to_supabase()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyfZ0D5kQKpW",
        "outputId": "25bca260-dd2e-4c89-b625-64c3e53270ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Supabase client initialized successfully!\n",
            "ðŸ§¹ Old data cleared\n",
            "ðŸš€ Data uploaded to Supabase!\n"
          ]
        }
      ]
    }
  ]
}